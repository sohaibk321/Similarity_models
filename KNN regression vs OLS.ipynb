{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neighbors\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['City', 'Population', 'Violent crime',\n",
       "       'Murder and nonnegligent manslaughter', 'Rape (revised definition)1',\n",
       "       'Rape (legacy definition)2', 'Robbery', 'Aggravated assault',\n",
       "       'Property crime', 'Burglary', 'Larceny- theft', 'Motor vehicle theft',\n",
       "       'Arson3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('table_8_offenses_known_to_law_enforcement_new_york_by_city_2013.csv')\n",
    "df.columns = df.columns.str.replace('\\n', ' ')\n",
    "df.drop(['Unnamed: 13'], axis=1, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fix = ['Population', 'Violent crime', 'Rape (legacy definition)2', 'Robbery', 'Aggravated assault', \n",
    "       'Property crime', 'Burglary', 'Larceny- theft', 'Motor vehicle theft']\n",
    "#fixing commas in the numeric columns\n",
    "for key in fix:\n",
    "    df[key] = pd.to_numeric(df[key].astype(str).str.replace(',',''), errors='coerce')\n",
    "df.dropna(axis=0, how='all', inplace=True)\n",
    "df.drop(df.loc[df['Population'].isnull()].index, inplace=True)\n",
    "df.drop('Rape (revised definition)1', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing outliers in the data\n",
    "label = df.loc[\n",
    "    df['Property crime']>(df['Property crime'].quantile(0.9))]\n",
    "df = df.drop(label.index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Population^2'] = df['Population'].map(lambda x: x**2)\n",
    "df['Murder'] = np.where(df['Murder and nonnegligent manslaughter']>0, 1, 0)\n",
    "df['Robbery_feature'] = np.where(df['Robbery']>0, 1, 0)\n",
    "df['below_avg_larceny'] = np.where(df['Larceny- theft']<df['Larceny- theft'].mean(), 1, 0)\n",
    "\n",
    "ndf = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting OLS regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coefficients: \n",
      " [[ 4.89200124e-08  1.34150969e+02  5.16283015e+01 -2.57870486e+02]]\n",
      "\n",
      "Intercept: \n",
      " [292.35045209]\n",
      "\n",
      "R-squared:\n",
      "0.7382864354875093\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and fit our model.\n",
    "regr = linear_model.LinearRegression()\n",
    "Y = ndf['Property crime'].values.reshape(-1, 1)\n",
    "X = ndf[['Population^2', 'Murder', 'Robbery_feature', 'below_avg_larceny']]\n",
    "regr.fit(X, Y)\n",
    "\n",
    "# Inspect the results.\n",
    "print('\\nCoefficients: \\n', regr.coef_)\n",
    "print('\\nIntercept: \\n', regr.intercept_)\n",
    "print('\\nR-squared:')\n",
    "print(regr.score(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Accuracy: 0.66 (+/- 0.29)\n",
      "Cross validation results:\n",
      " [0.79756909 0.68956991 0.72566842 0.38376397 0.70008135]\n"
     ]
    }
   ],
   "source": [
    "ols_score = cross_val_score(regr, X, Y, cv=5)\n",
    "print(\"Avg. Accuracy: %0.2f (+/- %0.2f)\" % (ols_score.mean(), ols_score.std() * 2))\n",
    "print(\"Cross validation results:\\n\", ols_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter coefficients\n",
      " Intercept            233.889544\n",
      "Population             0.005177\n",
      "Murder               111.839853\n",
      "Robbery_feature       40.304773\n",
      "below_avg_larceny   -219.100318\n",
      "dtype: float64\n",
      "\n",
      "P-Values\n",
      " Intercept            3.278312e-29\n",
      "Population           3.331712e-12\n",
      "Murder               1.474027e-07\n",
      "Robbery_feature      2.401201e-03\n",
      "below_avg_larceny    2.238071e-34\n",
      "dtype: float64\n",
      "\n",
      "R squared value\n",
      " 0.7647023866080331\n"
     ]
    }
   ],
   "source": [
    "ndf['Property_crime'] = ndf['Property crime']\n",
    "linear_formula = 'Property_crime ~ Population+Murder+Robbery_feature+below_avg_larceny'\n",
    "lm = smf.ols(linear_formula, ndf).fit()\n",
    "print(\"\\nParameter coefficients\\n\",lm.params)\n",
    "print(\"\\nP-Values\\n\",lm.pvalues)\n",
    "print(\"\\nR squared value\\n\", lm.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted Accuracy: 0.44 (+/- 0.31)\n",
      "Cross validation results:\n",
      " [0.65004077 0.17397157 0.46542781 0.44492267 0.48970406]\n",
      "R-squared:\n",
      " 0.6413457457840361\n",
      "\n",
      "Weighted Accuracy: 0.34 (+/- 0.39)\n",
      "Cross validation results:\n",
      " [0.61161573 0.01021231 0.38281527 0.42244886 0.28855554]\n",
      "R-squared:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "knn = neighbors.KNeighborsRegressor(n_neighbors=5)\n",
    "knn_w = neighbors.KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
    "Y = df['Property crime']\n",
    "X = df[['Population^2', 'Murder', 'Robbery_feature', 'below_avg_larceny']]\n",
    "knn.fit(X, Y)\n",
    "score_w = knn_w.fit(X, Y)\n",
    "\n",
    "score = cross_val_score(knn, X, Y, cv=5)\n",
    "score_w = cross_val_score(knn_w, X, Y, cv=5)\n",
    "print(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\n",
    "print(\"Cross validation results:\\n\", score)\n",
    "print(\"R-squared:\\n\", knn.score(X, Y))\n",
    "\n",
    "print(\"\\nWeighted Accuracy: %0.2f (+/- %0.2f)\" % (score_w.mean(), score_w.std() * 2))\n",
    "print(\"Cross validation results:\\n\", score_w)\n",
    "print(\"R-squared:\\n\", knn_w.score(X, Y))\n",
    "#clear signs of overfitting in the weighted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS results: \n",
      "\n",
      "Avg. Accuracy: 0.66 (+/- 0.29)\n",
      "Cross validation results:\n",
      " [0.79756909 0.68956991 0.72566842 0.38376397 0.70008135]\n",
      "R-squared:  0.7382864354875093\n",
      "\n",
      "KNN results:\n",
      "\n",
      "Unweighted Accuracy: 0.51 (+/- 0.27)\n",
      "Cross validation results:\n",
      " [0.67258691 0.28498522 0.47702886 0.48245067 0.6138753 ]\n",
      "R-squared:\n",
      " 0.6126324994915151\n"
     ]
    }
   ],
   "source": [
    "print(\"OLS results: \\n\")\n",
    "print(\"Avg. Accuracy: %0.2f (+/- %0.2f)\" % (ols_score.mean(), ols_score.std() * 2))\n",
    "print(\"Cross validation results:\\n\", ols_score)\n",
    "print(\"R-squared: \", regr.score(X, Y))\n",
    "\n",
    "print(\"\\nKNN results:\\n\")\n",
    "print(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\n",
    "print(\"Cross validation results:\\n\", score)\n",
    "print(\"R-squared:\\n\", knn.score(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS had the better results overall. Although this was run when the data was the same for both. Outliers were cleaned, but observations which affect the error distribution are still in the OLS model, so problems of multivariate normality and heteroscadasticity are not addressed. The assumption here is that if the model is going to be flawed for one, it might as well be for both to see what works. K was set to 10, but then moved down to 5 for a slightly better model. The KNN model is easier to fit and has fewer assumptions to satisfy, but the OLS, when properly preapred, will have better results. This might be because the data still experiences outliers, even after data cleaning which can affect the results. A weighted model was used to maybe address this but resulted in an extremely overfit model. OLS has the advantage of not being susceptible to outliers as badly during its regression, while KNN can have more drastic results because if the neighbors are outliers, then that specific prediction region is not generalizable. Due to not having too many observations in this dataset, it is wise not to set the k too high for KNN. This can somewhat counteract the effect of outliers but results in a slightly worse model. The biggest impact I see between the 2 models has to do with the non-parametric nature of KNN. With no assumptions to satify before the model can even begin, it can be easier to start off with this model, however the complexity of OLS allows for more fine tuned results if all assumptions are satisfied. The drawback to OLS is that observations have to either be deleted or imputed to run the model, which can take away some of the generalizability of the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
