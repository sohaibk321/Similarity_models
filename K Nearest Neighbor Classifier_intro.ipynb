{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn import neighbors \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbor (KNN) Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model learns through similarity. Looks for datapoints that are most similar to the value/observation we are tryng to predict. The first example is nearest neighbor. We want to see if a song is either 'jazz' or 'rock'. Our measurements are duraation in seconds of the song and the 'loudness' of the song measured in loudness units (can't use decibels since it isn't a linear measure). This method is best used on continuous variables as distance becomes a troubling measurement if data is not equidistant. Best to use only CONTINUOUS variables for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "music = pd.DataFrame()\n",
    "\n",
    "# Some data to play with.\n",
    "music['duration'] = [184, 134, 243, 186, 122, 197, 294, 382, 102, 264, \n",
    "                     205, 110, 307, 110, 397, 153, 190, 192, 210, 403,\n",
    "                     164, 198, 204, 253, 234, 190, 182, 401, 376, 102]\n",
    "music['loudness'] = [18, 34, 43, 36, 22, 9, 29, 22, 10, 24, \n",
    "                     20, 10, 17, 51, 7, 13, 19, 12, 21, 22,\n",
    "                     16, 18, 4, 23, 34, 19, 14, 11, 37, 42]\n",
    "\n",
    "# We know whether the songs in our training data are jazz or not.\n",
    "music['jazz'] = [ 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
    "                  0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
    "                  1, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
    "music['bpm'] = [ 105, 90, 78, 75, 120, 110, 80, 100, 105, 60,\n",
    "                  70, 105, 95, 70, 90, 105, 70, 75, 102, 100,\n",
    "                  100, 95, 90, 80, 90, 80, 100, 105, 70, 65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    music[music['jazz']==1].duration, \n",
    "    music[music['jazz']==1].loudness, color='red')\n",
    "\n",
    "plt.scatter(\n",
    "            music[music['jazz']==0].duration,\n",
    "            music[music['jazz']==0].loudness, color='blue')\n",
    "plt.legend(['Jazz', 'Rock'])\n",
    "plt.title('Jazz and Rock Characteristics')\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Loudness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the graph, we can see how the nearest neighbor model works. Whatever point we are trying to predict, it plots on that graph and see's which other data point is closest to it and uses it to make the prediction. We measure distance in 'Euclidean distance' which can be calculated using the pythagorean theorem. This works on an n-dimensional scale such that the distance can be measured using this format: sqrt( (x1-y1)^2 + (x2-y2)^2 + (x3-y3)^2 + ... + (xn-yn)^2) for an n-dimensional matrix of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neighbors = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "X = music[['loudness', 'duration']]\n",
    "Y = music.jazz\n",
    "neighbors.fit(X,Y)\n",
    "\n",
    "## Predict for a song with 24 loudness that's 190 seconds long.\n",
    "neighbors.predict([[24, 190]])\n",
    "\n",
    "#The result is 0, which is not jazz from our definition of the jazz variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbor is an extension of nearest neighbor, except instead of one point being used as the similarity comparison, it uses the k - nearest observations as comparisons. Let's try to remodel, except this time we'll use 5 neighbors for the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neighbors = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "X = music[['loudness', 'duration']]\n",
    "Y = music['jazz']\n",
    "neighbors.fit(X, Y)\n",
    "\n",
    "print(neighbors.predict([[24, 190]]))\n",
    "print(neighbors.predict_proba([[24,190]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "#mesh size\n",
    "h = 1\n",
    "\n",
    "# Plot the decision boundary. We assign a color to each point in the mesh.\n",
    "#This creates the min and max for loudness and duration\n",
    "x_min = X[:, 0].min() - .5\n",
    "x_max = X[:, 0].max() + .5\n",
    "y_min = X[:, 1].min() - .5\n",
    "y_max = X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(\n",
    "    np.arange(x_min, x_max, h),\n",
    "    np.arange(y_min, y_max, h)\n",
    ")\n",
    "#np.c_ pairs up values columnswise, so if you have 2 columns containing data, then it combines them to make one observation\n",
    "Z = neighbors.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot.\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(6, 4))\n",
    "plt.set_cmap(plt.cm.Paired)\n",
    "plt.pcolormesh(xx, yy, Z)\n",
    "\n",
    "# Add the training points to the plot.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "plt.xlabel('Loudness')\n",
    "plt.ylabel('Duration')\n",
    "plt.title('Mesh visualization')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar1 = [10,12,14,19,15,20,35,18]\n",
    "ar2 = [2,6,7,8,9,10,12,15]\n",
    "result = [0,0,1,0,1,1,0,1]\n",
    "def nearest_neighbor(outcome, var1, var2, pred1, pred2):\n",
    "    dist = []\n",
    "    for i in range(len(var1)):\n",
    "        dist.append((np.sqrt((pred1-var1[i])**2 + (pred2-var2[i])**2), outcome[i]))\n",
    "    return min(dist)[1]\n",
    "#testing model\n",
    "print(nearest_neighbor(result, ar1, ar2, 2, 6))\n",
    "\n",
    "#validating if model works\n",
    "print(nearest_neighbor(music['jazz'], music['loudness'], music['duration'], 24, 190))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we have 2 vastly different ranges for our continuous variables? Say one measure is number of floors and the other is square footage. Square footage would be spread much more than number of floors and calculating distance from these two variables can cause issues. In order to fix this, we can normalize the scale of the 2 variables. Normalization is the process by which we make 2 incommensurate measures comparable. 2 ways to do this\n",
    "\n",
    "1) Set the bounds to 0 to 1 and scale measurements within that scale. We could also use -1 to 1 but the difference is miniscule. Best applied if data has a linear relationship (more important) and if there are known limits to the dataset(secondary in importance)\n",
    "\n",
    "2) Calculate how far each point is from the mean. Basically convert everything into its Z-Score (x - mean)/std. Z-score represents how much the point is 'abnormal'/different from the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the vanilla version of KNN, each k-observations have an equal weight in determining the prediction decision. If the data is dense, then this isn't a problem as we don't want to try to draw information from small differences in distance.\n",
    "\n",
    "Sometimes, the k nearest observations are not similarly close to the test observation, so it can be a good idea to give a weight which is inversely proportional to the distance. This way, closer points will have a higher weight than those farther away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "neighbors = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "\n",
    "# Our input data frame will be the z-scores this time instead of raw data.\n",
    "X = pd.DataFrame({\n",
    "    'loudness': stats.zscore(music.loudness),\n",
    "    'duration': stats.zscore(music.duration)\n",
    "})\n",
    "\n",
    "# Fit our model.\n",
    "Y = music.jazz\n",
    "neighbors.fit(X, Y)\n",
    "\n",
    "# Arrays, not data frames, for the mesh.\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Mesh size.\n",
    "h = .01\n",
    "\n",
    "# Plot the decision boundary. We assign a color to each point in the mesh.\n",
    "x_min = X[:,0].min() - .5\n",
    "x_max = X[:,0].max() + .5\n",
    "y_min = X[:,1].min() - .5\n",
    "y_max = X[:,1].max() + .5\n",
    "xx, yy = np.meshgrid(\n",
    "    np.arange(x_min, x_max, h),\n",
    "    np.arange(y_min, y_max, h)\n",
    ")\n",
    "Z = neighbors.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(6, 4))\n",
    "plt.set_cmap(plt.cm.Paired)\n",
    "plt.pcolormesh(xx, yy, Z)\n",
    "\n",
    "# Add the training points to the plot.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "plt.xlabel('Loudness')\n",
    "plt.ylabel('Duration')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
